Processing amplicon-based resequencing data with amplimap
----------------------------------------------------------------------------

Experimental design
~~~~~~~~~~~~~~~~~~~~

This tutorial is based on *De novo and inherited loss-of-function
variants in TLK2: identification, clinical delineation and
genotype-phenotype evaluation of a distinct neurodevelopmental disorder*
by MRF Reijnders, KA Miller et al.

After identifying a possibly pathogenic mutation in TLK2, we wanted to
screen a large cohort of patients for anybody else who carries a
mutation in that gene. We designed 24 primers to capture all its exons and
used a 384-sample paired-end kit to sequence XXXXXX patients in a single
Illumina MiSeq run.

In this tutorial, we will look at how the data from this experiment was processed
using amplimap. We will be working with a subset of four samples, to speed
up processing times.

Analysis overview
~~~~~~~~~~~~~~~~~~~~

Starting from the raw sequencing reads, we would like to:

-  Trim off primer sequences
-  Align reads to the reference genome
-  Determine whether any target regions may have not been adequately
   covered in some of the samples
-  Call germline variants
-  Annotate variants with their consequences as well as allele
   frequencies and deleteriousness scores
-  Generate an overview table for manual inspection

Initial setup
~~~~~~~~~~~~~~~~~~~~

To run this tutorial amplimap needs to be installed and configured already.
Please see :ref:`installation` and  :ref:`configuration` for details.

In particular, you need to have the hg19 (GRCh37) reference genome and the
associated indices prepared for use with bwa and GATK (see :ref:`config-reference`)
and have Annovar installed (see :ref:`config-annovar`).

Preparing the working directory
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For every experiment that we want to process, we create a new working
directory. This will contain all the input files required, as well as
the output generated by amplimap. This makes it easy to keep track of
the data for each experiment, as well as to rerun analyses if required.

To create a directory, we use the standard ``mkdir`` unix command and
change into it with ``cd``:

::

    mkdir amplimap_wd1
    cd amplimap_wd1

All further commands should now be run inside this working directory.

reads_in
''''''''''''

The first input we need to provide to amplimap are of course the
sequencing data. These can be obtained directly from the sequencer as
``.fastq.gz`` files and should be placed in a directory called “reads_in”
(see :ref:`reads-in`).
You can download the sample data from this tutorial here. Extract the
.fastq.gz files to the reads_in directory.

You can use ls to check that the files are there:

::

    ls reads_in

This should display a list of 8 fastq.gz files, which represent read 1
and read 2 of four samples:

::

    xxxx

probes.csv
''''''''''''

Next, we need to provide a :ref:`probes-csv` file that describes the used
primer sequences and the regions they are supposed to capture. This can
be created with spreadsheet software such as Excel. However, we
recommend always checking the file manually using a plain text editor
such as ``nano`` or ``vim``, to make sure it is actually in the right
format.

Download the probes.csv file here and save it in the working directory.
You can run use the ``head`` command to view the first few lines of the file:

::

    head probes.csv

Which should display:

::

    xxxx

targets.csv
''''''''''''

We also want to describe the target regions that amplimap
should analyse in the :ref:`targets-csv` file. This helps speed up the
process, since any reads outside the targets can be ignored. It also
allows us to calculate the average coverage for each target in each
sample.

Download the targets.csv file here and save it in the working directory.
You can run use the ``head`` command to view the first few lines of the file:

::

    head targets.csv

Which should display:

::

    xxxx


config.yaml
''''''''''''

Finally, we create a config.yaml file to set some experiment-specific settings.
We could set a lot more options here (see :ref:`configuration`) but in this case
we will only specify the reference genome. All the other options will be left at
the default as specified in the :ref:`default-config`.

Create a new plain text file called config.yaml (for example using ``nano``
or ``vim``) and copy the following text into it:

::

    general:
      genome_name: "hg19"

This tells amplimap to run using the reference genome hg19, as specified in your
:ref:`default-config`.

Running amplimap
~~~~~~~~~~~~~~~~~~~~

Now we are ready to run amplimap!

First, we do a plain dry run to check that amplimap is installed
properly and can find the input files:

::

    amplimap

This should give us many lines of output, ending with this summary:

::

    xxxx

Then, we do another dry run, this time specifying the full analysis that
we want to perform. In our case, we want to obtain coverage values
(“coverages”) and annotated variant calls (“variants”). This will also
automatically run the other parts of the pipeline that are required,
such as trimming the primers and aligning reads to the genome:

::

    amplimap coverages variants

We should now get a longer list of jobs that will be run, like this:

::

    xxxx


If everything look okay we start amplimap:

::

    amplimap coverages variants

This will take a few minutes to complete. It would be much faster if we
ran jobs in parallel (for example using the cluster), but we are not
doing that for the purposes of this tutorial.

Output data
~~~~~~~~~~~~~~~~~~~~

All the output of amplimap will be placed inside the ``analysis`` directory.

Let's explore some of the output:

analysis/reads_parsed/
''''''''''''''''''''''''
This directory contains results from the first step of the pipeline which
identified primer arms in reads, trimmed them off and calculated some
run statistics.

analysis/bam/coverage/
''''''''''''''''''''''''
This directory contains the coverage information for each sample and each target.
The most comprehensive table to look at is coverages_long. In addition,
there are also various aggregate tables that, for example, tell you the minimum coverage in each target and
each sample (XXXX) or what fraction of the target was covered by at least one read (XXXX).

Let's have a look at the XXXX file:


We can see that samples X-X had good coverage for all targets. Target XX was 
better covered than XXX, but XXXXX. However, there was clearly a problem with
sample XX, which had very low coverage in XXX.

We might want to try sequencing this sample again.

variants/
''''''''''''''''''''''''
Here we can find the final output of the germline variant calling -
the annotated table of variants. The ``variants_summary.csv``
table includes all the information one would usually find in a VCF
file (chromosome, position, ref, alt, genotype, allele fraction, quality),
as well as various annotations from Annovar regarding the functional
impact and deleteriousness of the variant. There is also information
about the frequency of the variant in reference sets, such as 1000 Genomes,
gnomAD or ExAC.

There will be one line per sample and variant, so the same variant can appear
in multiple lines.

Let's have a look at a few examples:


In this case, we found one novel variant in XXX that looks deleterious:
XXX.
All the other variants (such as XXX) appear to be know polymorphisms, as
shown by their high allele frequencies in reference databases such as 1000 Genomes,
gnomAD, etc.













